# Unified CLI Commands Specification

**Status**: Active Implementation
**Owner**: Project Lead
**Version**: 2.0
**Last Updated**: 2025-01-03

## 1. Abstract

This document specifies the unified command-line interface refinements for the `pmat` toolkit, addressing critical usability issues identified in production usage. The primary objectives are to transform underperforming commands into robust analysis tools, establish consistent output behaviors with complete file path information, implement a comprehensive defect aggregation system via the enhanced `report` command, and achieve 95%+ test coverage with automated documentation validation.

## 2. Critical Issues to Address

### Production Defects Identified

1. **`pmat analyze big-o`**: No file output - users receive analysis without knowing which files are affected
2. **`pmat analyze comprehensive`**: Silent failure - command completes without producing any output
3. **`pmat analyze makefile`**: Incorrect default path handling - fails to find Makefile in current directory
4. **`pmat analyze complexity`**: Missing file context - shows metrics without file associations
5. **`--top-files N`**: Complete failure of ranking system - neither `N > 0` nor `N = 0` (all files) produces output
6. **`report` command**: Currently non-functional - requires complete reimplementation as defect aggregator

## 3. Command Enhancements and Reimplementation

### A. Transform `report` Command into Defect Aggregation Engine

The `report` command will be reimplemented as a comprehensive defect aggregation system that collects all quality issues across the codebase with full file path context.

**Core Architecture**:

```rust
pub struct DefectReport {
    pub metadata: ReportMetadata,
    pub defects: Vec<Defect>,
    pub summary: DefectSummary,
    pub file_index: BTreeMap<PathBuf, Vec<DefectId>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Defect {
    pub id: DefectId,
    pub severity: Severity,
    pub category: DefectCategory,
    pub file_path: PathBuf,          // Always absolute or project-relative
    pub line_start: u32,
    pub line_end: Option<u32>,
    pub column_start: Option<u32>,
    pub column_end: Option<u32>,
    pub message: String,
    pub rule_id: String,
    pub fix_suggestion: Option<String>,
    pub metrics: HashMap<String, f64>, // cyclomatic, cognitive, etc.
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum DefectCategory {
    Complexity,      // Cyclomatic/cognitive complexity violations
    TechnicalDebt,   // SATD markers
    DeadCode,        // Unreachable/unused code
    Duplication,     // Code clones
    Performance,     // O(nÂ²) or worse algorithms
    Architecture,    // Coupling/cohesion issues
    TestCoverage,    // Insufficient test coverage
}
```

**Default Behavior**: Generate comprehensive JSON with all defects

```bash
# Default: comprehensive JSON report
pmat report
# Outputs: defect-report-{timestamp}.json

# Format variants
pmat report --txt    # Human-readable text report
pmat report --md     # Markdown with tables and links
pmat report --csv    # Excel-compatible CSV
```

**Implementation**:

```rust
impl ReportCommand {
    pub async fn execute(&self, args: ReportArgs) -> Result<()> {
        let project = ProjectAnalyzer::new(&args.project_path)?;

        // Parallel defect collection pipeline
        let defects = self.collect_all_defects(&project).await?;

        // Generate report in requested format
        let output = match args.format {
            OutputFormat::Json => self.generate_json_report(&defects)?,
            OutputFormat::Text => self.generate_text_report(&defects)?,
            OutputFormat::Markdown => self.generate_markdown_report(&defects)?,
            OutputFormat::Csv => self.generate_csv_report(&defects)?,
        };

        // Write with automatic timestamping
        let filename = self.generate_filename(&args.format);
        fs::write(&filename, output)?;

        println!("Report generated: {}", filename);
        Ok(())
    }

    async fn collect_all_defects(&self, project: &Project) -> Result<Vec<Defect>> {
        // Parallel execution with controlled concurrency
        let semaphore = Arc::new(Semaphore::new(num_cpus::get()));

        let (complexity, satd, dead_code, duplication, perf, arch) = tokio::join!(
            self.analyze_complexity_defects(project, &semaphore),
            self.analyze_satd_defects(project, &semaphore),
            self.analyze_dead_code_defects(project, &semaphore),
            self.analyze_duplication_defects(project, &semaphore),
            self.analyze_performance_defects(project, &semaphore),
            self.analyze_architecture_defects(project, &semaphore),
        );

        // Merge and deduplicate
        let mut all_defects = Vec::with_capacity(10_000);
        all_defects.extend(complexity?);
        all_defects.extend(satd?);
        all_defects.extend(dead_code?);
        all_defects.extend(duplication?);
        all_defects.extend(perf?);
        all_defects.extend(arch?);

        // Sort by severity, then file, then line
        all_defects.sort_by_key(|d| (d.severity, d.file_path.clone(), d.line_start));

        Ok(all_defects)
    }
}
```

**JSON Output Structure**:

```json
{
  "metadata": {
    "tool": "pmat",
    "version": "0.27.0",
    "generated_at": "2025-01-03T10:30:00Z",
    "project_root": "/home/user/project",
    "total_files_analyzed": 246,
    "analysis_duration_ms": 3847
  },
  "summary": {
    "total_defects": 89,
    "by_severity": {
      "critical": 5,
      "high": 23,
      "medium": 45,
      "low": 16
    },
    "by_category": {
      "complexity": 34,
      "technical_debt": 22,
      "dead_code": 15,
      "duplication": 8,
      "performance": 6,
      "architecture": 4
    },
    "hotspot_files": [
      {
        "path": "src/parser/expression.rs",
        "defect_count": 12,
        "severity_score": 89.5
      }
    ]
  },
  "defects": [
    {
      "id": "CPLX-001",
      "severity": "high",
      "category": "complexity",
      "file_path": "src/parser/expression.rs",
      "line_start": 145,
      "line_end": 312,
      "column_start": 1,
      "column_end": 80,
      "message": "Function 'parse_binary_expression' has cyclomatic complexity of 42 (threshold: 20)",
      "rule_id": "cyclomatic-complexity",
      "fix_suggestion": "Consider extracting operator precedence handling into separate methods",
      "metrics": {
        "cyclomatic": 42,
        "cognitive": 38,
        "halstead_effort": 2450.7,
        "lines_of_code": 167
      }
    }
  ],
  "file_index": {
    "src/parser/expression.rs": ["CPLX-001", "CPLX-002", "SATD-003"],
    "src/lexer/tokenizer.rs": ["DEAD-004", "PERF-005"]
  }
}
```

**CSV Output Structure**:

```csv
id,severity,category,file_path,line_start,line_end,message,rule_id,cyclomatic,cognitive
CPLX-001,high,complexity,src/parser/expression.rs,145,312,"Function 'parse_binary_expression' has cyclomatic complexity of 42",cyclomatic-complexity,42,38
SATD-002,medium,technical_debt,src/cache/lru.rs,67,67,"TODO: Implement proper cache eviction strategy",satd-todo,,
```

**Markdown Output Features**:

```rust
impl MarkdownReportGenerator {
    fn generate(&self, defects: &[Defect]) -> String {
        let mut md = String::with_capacity(100_000);

        // Executive summary with metrics
        writeln!(md, "# Code Quality Report\n");
        writeln!(md, "Generated: {}", Utc::now().format("%Y-%m-%d %H:%M:%S UTC"));
        writeln!(md, "\n## Executive Summary\n");

        // Severity distribution chart (ASCII)
        writeln!(md, "### Severity Distribution\n");
        writeln!(md, "```");
        writeln!(md, "Critical â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 5 (5.6%)");
        writeln!(md, "High     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 23 (25.8%)");
        writeln!(md, "Medium   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45 (50.6%)");
        writeln!(md, "Low      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 16 (18.0%)");
        writeln!(md, "```\n");

        // Top 10 hotspot files with GitHub links
        writeln!(md, "### Top 10 Hotspot Files\n");
        writeln!(md, "| Rank | File | Defects | Severity Score |");
        writeln!(md, "|------|------|---------|----------------|");

        for (i, hotspot) in self.compute_hotspots(defects).iter().take(10).enumerate() {
            writeln!(md, "| {} | [{}]({}) | {} | {:.1} |",
                i + 1,
                hotspot.file.file_name().unwrap().to_string_lossy(),
                hotspot.file.display(),
                hotspot.defect_count,
                hotspot.severity_score
            );
        }

        // Detailed defects by category
        writeln!(md, "\n## Detailed Findings\n");

        for category in DefectCategory::iter() {
            let category_defects: Vec<_> = defects.iter()
                .filter(|d| d.category == category)
                .collect();

            if !category_defects.is_empty() {
                writeln!(md, "### {} ({} issues)\n", category, category_defects.len());

                for defect in category_defects {
                    writeln!(md, "#### {}:{}-{}",
                        defect.file_path.display(),
                        defect.line_start,
                        defect.line_end.unwrap_or(defect.line_start)
                    );
                    writeln!(md, "\n**{}** - {}\n", defect.severity, defect.message);

                    if let Some(fix) = &defect.fix_suggestion {
                        writeln!(md, "> ðŸ’¡ **Suggestion**: {}\n", fix);
                    }
                }
            }
        }

        md
    }
}
```

### B. Comprehensive Report Generation via Enhanced Subcommands

Each `analyze` subcommand will support comprehensive reporting that feeds into the unified `report` command:

```rust
pub trait DefectAnalyzer: Send + Sync {
    type Config: Default + Clone;

    async fn analyze(&self, project: &Project, config: Self::Config) -> Result<Vec<Defect>>;

    fn category(&self) -> DefectCategory;

    fn supports_incremental(&self) -> bool { false }
}

// Example implementation for complexity analyzer
impl DefectAnalyzer for ComplexityAnalyzer {
    type Config = ComplexityConfig;

    async fn analyze(&self, project: &Project, config: Self::Config) -> Result<Vec<Defect>> {
        let mut defects = Vec::new();

        for file in project.source_files() {
            let ast = self.parse_file(&file).await?;

            for function in ast.functions() {
                let metrics = self.compute_metrics(&function)?;

                if metrics.cyclomatic > config.max_cyclomatic {
                    defects.push(Defect {
                        id: DefectId::new("CPLX", defects.len()),
                        severity: self.severity_for_complexity(metrics.cyclomatic),
                        category: DefectCategory::Complexity,
                        file_path: file.path().to_path_buf(),
                        line_start: function.span.start.line,
                        line_end: Some(function.span.end.line),
                        column_start: Some(function.span.start.column),
                        column_end: Some(function.span.end.column),
                        message: format!(
                            "Function '{}' has cyclomatic complexity of {} (threshold: {})",
                            function.name, metrics.cyclomatic, config.max_cyclomatic
                        ),
                        rule_id: "cyclomatic-complexity".to_string(),
                        fix_suggestion: self.suggest_refactoring(&function, &metrics),
                        metrics: hashmap! {
                            "cyclomatic".to_string() => metrics.cyclomatic as f64,
                            "cognitive".to_string() => metrics.cognitive as f64,
                            "halstead_effort".to_string() => metrics.halstead_effort,
                        },
                    });
                }
            }
        }

        Ok(defects)
    }

    fn category(&self) -> DefectCategory {
        DefectCategory::Complexity
    }

    fn supports_incremental(&self) -> bool {
        true // Can analyze only changed files
    }
}
```

## 4. Unified Analyze Subcommand Behaviors

### C. Core Behavioral Specifications

#### 1. Universal Default: File-Centric Output

**Every `analyze` subcommand MUST provide complete file context**:

```rust
pub struct AnalysisResult {
    pub file_path: PathBuf,      // Canonical path relative to project root
    pub absolute_path: PathBuf,  // Absolute filesystem path
    pub line_range: LineRange,
    pub metrics: BTreeMap<String, MetricValue>,
    pub context: AnalysisContext,
}

#[derive(Debug, Clone)]
pub struct LineRange {
    pub start: LineInfo,
    pub end: Option<LineInfo>,
}

#[derive(Debug, Clone)]
pub struct LineInfo {
    pub line: u32,
    pub column: u32,
    pub byte_offset: usize,
}
```

#### 2. Robust `--top-files` Implementation

The ranking system must handle edge cases correctly:

```rust
pub struct FileRankingEngine {
    scorer: Box<dyn FileScorer + Send + Sync>,
    cache: Arc<DashMap<PathBuf, f64>>,
}

impl FileRankingEngine {
    pub fn rank_files(&self, results: Vec<AnalysisResult>, limit: usize) -> Vec<RankedFile> {
        // Compute composite scores with caching
        let mut scored: Vec<_> = results.into_par_iter()
            .map(|result| {
                let score = self.cache.get(&result.file_path)
                    .map(|s| *s)
                    .unwrap_or_else(|| {
                        let s = self.scorer.compute_score(&result);
                        self.cache.insert(result.file_path.clone(), s);
                        s
                    });

                RankedFile {
                    rank: 0, // Will be set after sorting
                    score,
                    path: result.file_path.clone(),
                    result,
                }
            })
            .collect();

        // Stable sort for deterministic output
        scored.par_sort_by(|a, b| {
            b.score.partial_cmp(&a.score)
                .unwrap_or(Ordering::Equal)
                .then_with(|| a.path.cmp(&b.path)) // Secondary sort by path
        });

        // Apply limit: 0 means all files
        let take_count = if limit == 0 { scored.len() } else { limit.min(scored.len()) };
        scored.truncate(take_count);

        // Assign ranks
        for (i, file) in scored.iter_mut().enumerate() {
            file.rank = i + 1;
        }

        scored
    }
}
```

#### 3. Standardized Table Output

All commands must support consistent table formatting:

```rust
pub struct TableFormatter {
    columns: Vec<Column>,
    alignment: Vec<Alignment>,
}

impl TableFormatter {
    pub fn format_analysis_table(results: &[RankedFile]) -> String {
        let mut table = Table::new();

        // Standard columns for all analysis types
        table.set_header(vec![
            Cell::new("RANK").with_style(Attr::Bold),
            Cell::new("SCORE").with_style(Attr::Bold),
            Cell::new("FILE:LINE").with_style(Attr::Bold),
            Cell::new("METRIC").with_style(Attr::Bold),
            Cell::new("DETAILS").with_style(Attr::Bold),
        ]);

        for ranked in results {
            let file_location = format!(
                "{}:{}-{}",
                ranked.path.display(),
                ranked.result.line_range.start.line,
                ranked.result.line_range.end
                    .as_ref()
                    .map(|e| e.line.to_string())
                    .unwrap_or_else(|| "EOF".to_string())
            );

            let metrics = ranked.result.metrics.iter()
                .map(|(k, v)| format!("{}={}", k, v))
                .collect::<Vec<_>>()
                .join(", ");

            table.add_row(vec![
                Cell::new(&ranked.rank.to_string()),
                Cell::new(&format!("{:.1}", ranked.score)),
                Cell::new(&file_location).with_style(Attr::ForegroundColor(color::BLUE)),
                Cell::new(&metrics),
                Cell::new(&ranked.result.context.description),
            ]);
        }

        table.to_string()
    }
}
```

### D. Specific Command Fixes

#### `analyze big-o`

**Fix**: Implement comprehensive file-aware Big-O analysis

```rust
pub struct BigOAnalyzer {
    ast_analyzer: AstAnalyzer,
    loop_detector: LoopPatternDetector,
    recursion_analyzer: RecursionAnalyzer,
}

impl BigOAnalyzer {
    pub async fn analyze(&self, project: &Project) -> Result<BigOAnalysis> {
        let mut results = Vec::with_capacity(1000);

        // Parallel file analysis with bounded concurrency
        let semaphore = Arc::new(Semaphore::new(num_cpus::get() * 2));
        let mut tasks = JoinSet::new();

        for file in project.source_files() {
            let sem = semaphore.clone();
            let analyzer = self.clone();

            tasks.spawn(async move {
                let _permit = sem.acquire().await.unwrap();
                analyzer.analyze_file(file).await
            });
        }

        while let Some(result) = tasks.join_next().await {
            results.extend(result??);
        }

        Ok(BigOAnalysis {
            results,
            summary: self.compute_summary(&results),
        })
    }

    async fn analyze_file(&self, file: &SourceFile) -> Result<Vec<BigOResult>> {
        let ast = self.ast_analyzer.parse(file).await?;
        let mut results = Vec::new();

        for function in ast.functions() {
            let loops = self.loop_detector.find_loops(&function);
            let recursion = self.recursion_analyzer.detect_recursion(&function);

            let complexity = self.compute_complexity(&loops, &recursion);

            if let Some(complexity) = complexity {
                results.push(BigOResult {
                    file_path: file.path().to_path_buf(),
                    function_name: function.name.clone(),
                    line_start: function.span.start.line,
                    line_end: function.span.end.line,
                    time_complexity: complexity.time,
                    space_complexity: complexity.space,
                    confidence: complexity.confidence,
                    evidence: complexity.evidence,
                });
            }
        }

        Ok(results)
    }
}
```

#### `analyze comprehensive`

**Fix**: Implement actual comprehensive pipeline with progress reporting

```rust
pub struct ComprehensiveAnalyzer {
    analyzers: Vec<Box<dyn DefectAnalyzer>>,
    progress_sink: Arc<dyn ProgressReporter>,
}

impl ComprehensiveAnalyzer {
    pub async fn analyze(&self, project: &Project) -> Result<ComprehensiveReport> {
        let total_steps = self.analyzers.len();
        let mut all_defects = Vec::with_capacity(1000);

        for (i, analyzer) in self.analyzers.iter().enumerate() {
            self.progress_sink.report(ProgressEvent {
                step: i + 1,
                total: total_steps,
                message: format!("Running {} analysis", analyzer.category()),
                percentage: (i as f32 / total_steps as f32) * 100.0,
            });

            match analyzer.analyze(project, Default::default()).await {
                Ok(defects) => all_defects.extend(defects),
                Err(e) => {
                    eprintln!("Warning: {} analysis failed: {}", analyzer.category(), e);
                    // Continue with other analyzers
                }
            }
        }

        // Build comprehensive file index
        let mut file_summaries: BTreeMap<PathBuf, FileSummary> = BTreeMap::new();

        for defect in &all_defects {
            file_summaries
                .entry(defect.file_path.clone())
                .or_insert_with(FileSummary::default)
                .add_defect(defect);
        }

        Ok(ComprehensiveReport {
            timestamp: Utc::now(),
            project_root: project.root().to_path_buf(),
            total_files: file_summaries.len(),
            total_defects: all_defects.len(),
            defects: all_defects,
            file_summaries,
            quality_score: self.compute_quality_score(&file_summaries),
        })
    }
}
```

#### `analyze makefile`

**Fix**: Implement robust Makefile discovery with fallback strategies

```rust
pub struct MakefileLocator {
    search_patterns: Vec<&'static str>,
    max_depth: usize,
}

impl Default for MakefileLocator {
    fn default() -> Self {
        Self {
            search_patterns: vec![
                "Makefile",
                "makefile",
                "GNUmakefile",
                "Makefile.*",
                "*.mk",
            ],
            max_depth: 3,
        }
    }
}

impl MakefileLocator {
    pub fn find_makefiles(&self, root: &Path) -> Result<Vec<PathBuf>> {
        let mut makefiles = Vec::new();
        let mut visited = HashSet::new();

        // BFS with depth limit
        let mut queue = VecDeque::new();
        queue.push_back((root.to_path_buf(), 0));

        while let Some((dir, depth)) = queue.pop_front() {
            if depth > self.max_depth || !visited.insert(dir.clone()) {
                continue;
            }

            // Check all patterns in current directory
            for pattern in &self.search_patterns {
                let glob_pattern = dir.join(pattern);

                for entry in glob::glob(&glob_pattern.to_string_lossy())? {
                    if let Ok(path) = entry {
                        if path.is_file() {
                            makefiles.push(path);
                        }
                    }
                }
            }

            // Add subdirectories to queue
            if let Ok(entries) = fs::read_dir(&dir) {
                for entry in entries.filter_map(Result::ok) {
                    let path = entry.path();
                    if path.is_dir() && !self.is_ignored(&path) {
                        queue.push_back((path, depth + 1));
                    }
                }
            }
        }

        if makefiles.is_empty() {
            return Err(anyhow!(
                "No Makefiles found in {} (searched {} patterns up to depth {})",
                root.display(),
                self.search_patterns.len(),
                self.max_depth
            ));
        }

        // Sort by proximity to root
        makefiles.sort_by_key(|p| p.components().count());

        Ok(makefiles)
    }

    fn is_ignored(&self, path: &Path) -> bool {
        path.file_name()
            .and_then(|n| n.to_str())
            .map(|n| matches!(n, "target" | "node_modules" | ".git" | "vendor"))
            .unwrap_or(false)
    }
}
```

### E. Quality Gate Integration

All commands must support CI/CD-friendly quality gates:

```rust
pub struct QualityGateEngine {
    thresholds: QualityThresholds,
    output_format: QualityGateFormat,
}

impl QualityGateEngine {
    pub fn evaluate(&self, analysis: &dyn QualityGateAnalysis) -> QualityGateResult {
        let violations = analysis.check_violations(&self.thresholds);

        let result = QualityGateResult {
            passed: violations.is_empty(),
            violations,
            metrics: analysis.quality_metrics(),
            timestamp: Utc::now(),
        };

        // Set process exit code for CI/CD
        if !result.passed {
            std::process::exit(1);
        }

        result
    }

    pub fn format_output(&self, result: &QualityGateResult) -> String {
        match self.output_format {
            QualityGateFormat::Human => self.format_human(result),
            QualityGateFormat::Json => serde_json::to_string_pretty(result).unwrap(),
            QualityGateFormat::Junit => self.format_junit(result),
            QualityGateFormat::Sarif => self.format_sarif(result),
        }
    }
}
```

## 5. Implementation Requirements

### F. Rust Documentation Coverage

**Target**: 95%+ documentation with examples and invariants

```rust
// server/src/lib.rs
#![deny(missing_docs)]
#![deny(rustdoc::missing_crate_level_docs)]
#![warn(rustdoc::missing_doc_code_examples)]

//! # PMAT - High-Performance Code Analysis Toolkit
//!
//! A symbolic AI system for comprehensive code quality analysis implementing
//! the Model Context Protocol with zero-copy I/O and lock-free concurrency.
//!
//! ## Architecture
//!
//! The system employs a three-tier analysis pipeline:
//!
//! 1. **AST Parsing Layer**: Language-specific parsers with incremental caching
//! 2. **Analysis Engine Layer**: Parallel defect detection with SIMD optimization
//! 3. **Report Generation Layer**: Multi-format output with streaming serialization
//!
//! ## Performance Characteristics
//!
//! - **Parsing**: O(n) with ~250 files/sec throughput
//! - **Analysis**: O(n log n) worst case, typically O(n) with caching
//! - **Memory**: ~500KB per file analyzed, bounded by available RAM
//!
//! ## Example
//!
//! ```rust
//! use pmat::{ProjectAnalyzer, DefectReport, OutputFormat};
//!
//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {
//! let analyzer = ProjectAnalyzer::new(".")?;
//! let report = analyzer.comprehensive_analysis().await?;
//!
//! // Generate report in multiple formats
//! report.write_json("defects.json")?;
//! report.write_markdown("REPORT.md")?;
//! # Ok(())
//! # }
//! ```

/// Comprehensive defect analyzer with pluggable analysis engines.
///
/// # Invariants
///
/// - File paths are always canonical and relative to project root
/// - Defect IDs are globally unique within a report
/// - Analysis is deterministic given identical input
///
/// # Performance
///
/// The analyzer uses bounded parallelism with `num_cpus * 2` workers
/// to balance CPU utilization with memory pressure.
pub struct DefectAnalyzer {
    engines: Vec<Box<dyn AnalysisEngine>>,
    config: AnalyzerConfig,
}
```

### G. Comprehensive Test Strategy

#### 1. Output Format Validation

```rust
#[cfg(test)]
mod report_format_tests {
    use super::*;
    use serde_json::Value;
    use csv::Reader;

    #[test]
    fn test_json_report_schema() {
        let report = create_test_report();
        let json = report.to_json().unwrap();
        let value: Value = serde_json::from_str(&json).unwrap();

        // Validate schema structure
        assert!(value["metadata"].is_object());
        assert!(value["metadata"]["version"].is_string());
        assert!(value["defects"].is_array());

        // Every defect must have complete file path
        for defect in value["defects"].as_array().unwrap() {
            assert!(defect["file_path"].is_string());
            let path = defect["file_path"].as_str().unwrap();
            assert!(!path.is_empty());
            assert!(path.ends_with(".rs") || path.ends_with(".ts") || path.ends_with(".py"));

            assert!(defect["line_start"].as_u64().unwrap() > 0);
        }
    }

    #[test]
    fn test_csv_report_parsing() {
        let report = create_test_report();
        let csv = report.to_csv().unwrap();

        let mut reader = Reader::from_reader(csv.as_bytes());
        let headers = reader.headers().unwrap();

        // Required columns
        assert!(headers.iter().any(|h| h == "file_path"));
        assert!(headers.iter().any(|h| h == "line_start"));
        assert!(headers.iter().any(|h| h == "severity"));

        // All rows must have valid file paths
        for result in reader.records() {
            let record = result.unwrap();
            let file_path = &record[headers.iter().position(|h| h == "file_path").unwrap()];
            assert!(!file_path.is_empty());
            assert!(PathBuf::from(file_path).is_relative());
        }
    }
}
```

#### 2. File Ranking Correctness

```rust
#[cfg(test)]
mod ranking_tests {
    use super::*;
    use proptest::prelude::*;

    proptest! {
        #[test]
        fn test_ranking_stability(
            files in prop::collection::vec(
                (any::<String>(), 0.0f64..1000.0f64),
                1..1000
            )
        ) {
            let results: Vec<_> = files.into_iter()
                .enumerate()
                .map(|(i, (name, score))| {
                    create_mock_result(&format!("{}.rs", name), score, i as u32)
                })
                .collect();

            let engine = FileRankingEngine::new(Box::new(SimpleScorer));

            // Test limit = 0 (all files)
            let all_ranked = engine.rank_files(results.clone(), 0);
            prop_assert_eq!(all_ranked.len(), results.len());

            // Test limit > 0
            for limit in [1, 10, 50, 100, 500] {
                let ranked = engine.rank_files(results.clone(), limit);
                prop_assert!(ranked.len() <= limit);
                prop_assert!(ranked.len() <= results.len());

                // Verify descending order
                for window in ranked.windows(2) {
                    prop_assert!(window[0].score >= window[1].score);
                }

                // Verify all have valid paths
                for r in &ranked {
                    prop_assert!(!r.path.as_os_str().is_empty());
                }
            }
        }

        #[test]
        fn test_ranking_determinism(seed: u64) {
            // Generate random but deterministic data
            let mut rng = StdRng::seed_from_u64(seed);
            let files: Vec<_> = (0..100)
                .map(|i| {
                    let score = rng.gen_range(0.0..100.0);
                    create_mock_result(&format!("file{}.rs", i), score, i)
                })
                .collect();

            let engine = FileRankingEngine::new(Box::new(SimpleScorer));

            // Multiple runs should produce identical results
            let run1 = engine.rank_files(files.clone(), 10);
            let run2 = engine.rank_files(files.clone(), 10);

            prop_assert_eq!(run1.len(), run2.len());
            for (a, b) in run1.iter().zip(run2.iter()) {
                prop_assert_eq!(a.path, b.path);
                prop_assert_eq!(a.rank, b.rank);
                prop_assert!((a.score - b.score).abs() < f64::EPSILON);
            }
        }
    }
}
```

#### 3. CLI Output Integration Tests

```rust
#[test]
fn test_report_command_output() {
    let temp_dir = tempdir().unwrap();
    let project_path = create_test_project(&temp_dir);

    // Test default JSON output
    let output = Command::new(env!("CARGO_BIN_EXE_pmat"))
        .args(&["report"])
        .current_dir(&project_path)
        .output()
        .expect("Failed to execute");

    assert!(output.status.success());

    // Find generated report file
    let report_files: Vec<_> = fs::read_dir(&project_path)
        .unwrap()
        .filter_map(Result::ok)
        .map(|e| e.path())
        .filter(|p| {
            p.file_name()
                .and_then(|n| n.to_str())
                .map(|n| n.starts_with("defect-report-") && n.ends_with(".json"))
                .unwrap_or(false)
        })
        .collect();

    assert_eq!(report_files.len(), 1);

    // Validate JSON content
    let content = fs::read_to_string(&report_files[0]).unwrap();
    let json: Value = serde_json::from_str(&content).unwrap();

    assert!(json["metadata"]["tool"].as_str() == Some("pmat"));
    assert!(json["defects"].as_array().unwrap().len() > 0);
}

#[test]
fn test_report_format_flags() {
    let temp_dir = tempdir().unwrap();
    let project_path = create_test_project(&temp_dir);

    for (flag, extension) in &[("--txt", ".txt"), ("--md", ".md"), ("--csv", ".csv")] {
        let output = Command::new(env!("CARGO_BIN_EXE_pmat"))
            .args(&["report", flag])
            .current_dir(&project_path)
            .output()
            .expect("Failed to execute");

        assert!(output.status.success());

        // Verify file with correct extension was created
        let has_file = fs::read_dir(&project_path)
            .unwrap()
            .any(|e| {
                e.ok()
                    .and_then(|e| e.path().extension().map(|ext| ext == extension.trim_start_matches('.')))
                    .unwrap_or(false)
            });

        assert!(has_file, "No {} file created", extension);
    }
}
```

### H. Performance Requirements and Benchmarks

```rust
#[cfg(all(test, not(debug_assertions)))]
mod benchmarks {
    use super::*;
    use criterion::{black_box, criterion_group, criterion_main, Criterion};

    fn benchmark_report_generation(c: &mut Criterion) {
        let project = create_large_test_project(); // 1000+ files

        c.bench_function("report_json_generation", |b| {
            b.iter(|| {
                let analyzer = ProjectAnalyzer::new(&project).unwrap();
                let report = black_box(analyzer.generate_report(OutputFormat::Json));
                std::hint::black_box(report);
            })
        });

        c.bench_function("report_csv_generation", |b| {
            b.iter(|| {
                let analyzer = ProjectAnalyzer::new(&project).unwrap();
                let report = black_box(analyzer.generate_report(OutputFormat::Csv));
                std::hint::black_box(report);
            })
        });
    }

    criterion_group!(benches, benchmark_report_generation);
    criterion_main!(benches);
}
```

### Performance Targets

| Operation | File Count | Target p50 | Target p99 | Memory |
|-----------|------------|------------|------------|---------|
| `report` (JSON) | 1000 | <3s | <5s | <500MB |
| `report` (CSV) | 1000 | <2s | <3s | <300MB |
| `report` (Markdown) | 1000 | <4s | <6s | <400MB |
| `analyze complexity --top-files 10` | 10000 | <5s | <8s | <1GB |

## 6. Acceptance Criteria

This initiative is considered **complete** when:

1. âœ… The `report` command generates comprehensive JSON by default with all defect categories
2. âœ… `report --txt`, `--md`, and `--csv` produce correctly formatted output
3. âœ… Every defect in reports includes complete file path and line information
4. âœ… All `analyze` subcommands show file information by default
5. âœ… `--top-files N` works correctly for all values including 0
6. âœ… `analyze comprehensive` produces actual comprehensive output
7. âœ… `analyze makefile` finds Makefiles using intelligent search
8. âœ… Documentation coverage exceeds 95% on docs.rs
9. âœ… All CLI examples in documentation pass automated tests
10. âœ… Property tests validate ranking algorithms and output formats
11. âœ… Performance benchmarks meet specified targets
12. âœ… Quality gate integration works for all commands with proper exit codes

## 7. Migration Guide

### For Users

```bash
# Generate comprehensive defect report (default JSON)
pmat report

# Generate human-readable text report
pmat report --txt

# Generate markdown report for GitHub
pmat report --md

# Generate CSV for Excel analysis
pmat report --csv

# All analyze commands now show files by default
pmat analyze complexity --top-files 10
pmat analyze big-o  # Now shows file paths!
```

### For CI/CD Pipelines

```yaml
# Comprehensive quality check with report
- name: Generate quality report
  run: |
    pmat report --md
    mv defect-report-*.md ${{ github.workspace }}/quality-report.md

# Quality gate with proper exit codes
- name: Quality gate check
  run: |
    pmat analyze complexity --quality-gate --max-cyclomatic 20 || exit_code=$?
    pmat analyze satd --quality-gate --max-items 50 || exit_code=$?
    exit ${exit_code:-0}
```

---
## 8. GitHub Actions CI/CD Integration

### Production-Grade Workflow Examples

This section provides battle-tested GitHub Actions workflows optimized for high-performance CI/CD pipelines with intelligent caching, parallel execution, and deterministic builds.

#### A. Pull Request Quality Gates

**`.github/workflows/pr-quality-gates.yml`**

```yaml
name: PR Quality Gates
on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - '**/*.rs'
      - '**/Cargo.toml'
      - '**/Cargo.lock'

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0  # Disable for reproducible builds
  RUSTFLAGS: "-D warnings -C target-cpu=native"

jobs:
  quality-analysis:
    name: Quality Analysis (${{ matrix.check }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        check:
          - complexity
          - satd
          - dead-code
          - duplication
          - architecture

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for churn analysis

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache pmat binary
        id: cache-pmat
        uses: actions/cache@v4
        with:
          path: ~/.cargo/bin/pmat
          key: pmat-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}

      - name: Install pmat
        if: steps.cache-pmat.outputs.cache-hit != 'true'
        run: cargo install --path server --locked

      - name: Run ${{ matrix.check }} analysis
        run: |
          case "${{ matrix.check }}" in
            complexity)
              pmat analyze complexity --quality-gate --max-cyclomatic 20 --format json > complexity.json
              ;;
            satd)
              pmat analyze satd --quality-gate --max-items 50 --format json > satd.json
              ;;
            dead-code)
              pmat analyze dead-code --quality-gate --max-percentage 5 --format json > dead-code.json
              ;;
            duplication)
              pmat analyze duplicates --quality-gate --max-duplication 3 --format json > duplication.json
              ;;
            architecture)
              pmat analyze dag --quality-gate --max-coupling 10 --format json > architecture.json
              ;;
          esac

      - name: Upload analysis results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: analysis-${{ matrix.check }}
          path: ${{ matrix.check }}.json
          retention-days: 7

  comprehensive-report:
    name: Generate Comprehensive Report
    runs-on: ubuntu-latest
    needs: quality-analysis
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: analysis-results

      - name: Install pmat
        run: |
          curl -sSfL https://raw.githubusercontent.com/paiml/paiml-mcp-agent-toolkit/master/scripts/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Generate unified report
        run: |
          pmat report --md > quality-report.md
          pmat report --json > quality-report.json

      - name: Comment PR with report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('quality-report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' && comment.body.includes('## ðŸ“Š Code Quality Report')
            );

            const body = `## ðŸ“Š Code Quality Report\n\n${report}\n\n*Generated at: ${new Date().toISOString()}*`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Check quality gates
        run: |
          # Parse JSON report for pass/fail
          python3 -c "
          import json
          import sys

          with open('quality-report.json') as f:
              report = json.load(f)

          total_defects = report['summary']['total_defects']
          critical = report['summary']['by_severity'].get('critical', 0)
          high = report['summary']['by_severity'].get('high', 0)

          if critical > 0:
              print(f'âŒ FAILED: {critical} critical defects found')
              sys.exit(1)
          elif high > 10:
              print(f'âŒ FAILED: {high} high severity defects exceed threshold (10)')
              sys.exit(1)
          elif total_defects > 100:
              print(f'âš ï¸  WARNING: {total_defects} total defects found')
              sys.exit(0)
          else:
              print(f'âœ… PASSED: {total_defects} defects within acceptable limits')
              sys.exit(0)
          "
```

#### B. Nightly Comprehensive Analysis

**`.github/workflows/nightly-analysis.yml`**

```yaml
name: Nightly Comprehensive Analysis
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily
  workflow_dispatch:
    inputs:
      depth:
        description: 'Analysis depth (1-3)'
        required: false
        default: '3'

jobs:
  comprehensive-analysis:
    name: Deep Code Analysis
    runs-on: ubuntu-latest-16-cores  # High-performance runner
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup build environment
        run: |
          # Install system dependencies for AST parsers
          sudo apt-get update
          sudo apt-get install -y build-essential pkg-config libssl-dev

          # Set up swap for memory-intensive analysis
          sudo fallocate -l 8G /swapfile
          sudo chmod 600 /swapfile
          sudo mkswap /swapfile
          sudo swapon /swapfile

      - name: Install Rust with caching
        uses: dtolnay/rust-toolchain@stable

      - name: Smart dependency caching
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true
          cache-all-crates: true

      - name: Build pmat with optimizations
        run: |
          cargo build --release --locked
          cp target/release/pmat /usr/local/bin/

      - name: Run comprehensive analysis
        env:
          RUST_LOG: pmat=debug
          PMAT_CACHE_DIR: ${{ runner.temp }}/pmat-cache
        run: |
          # Warm up cache with parallel pre-analysis
          pmat analyze complexity --top-files 0 &
          pmat analyze satd &
          pmat analyze dead-code &
          wait

          # Full comprehensive analysis
          pmat analyze comprehensive \
            --include-duplicates \
            --include-dead-code \
            --include-defects \
            --include-complexity \
            --include-tdg \
            --perf \
            --format json \
            --output comprehensive-analysis.json

      - name: Generate multiple report formats
        run: |
          pmat report --json > defect-report.json
          pmat report --md > QUALITY_REPORT.md
          pmat report --csv > defect-report.csv

          # Generate SARIF for GitHub Security tab
          pmat analyze comprehensive --format sarif > analysis.sarif

      - name: Upload SARIF to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: analysis.sarif
          category: pmat-analysis

      - name: Store analysis artifacts
        uses: actions/upload-artifact@v4
        with:
          name: nightly-analysis-${{ github.run_number }}
          path: |
            comprehensive-analysis.json
            defect-report.*
            QUALITY_REPORT.md
          retention-days: 90

      - name: Update metrics dashboard
        env:
          METRICS_API_KEY: ${{ secrets.METRICS_API_KEY }}
        run: |
          # Push metrics to monitoring system
          curl -X POST https://metrics.example.com/api/v1/quality \
            -H "Authorization: Bearer $METRICS_API_KEY" \
            -H "Content-Type: application/json" \
            -d @comprehensive-analysis.json
```

#### C. Release Quality Validation

**`.github/workflows/release-validation.yml`**

```yaml
name: Release Quality Validation
on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  validate-release-quality:
    name: Validate Release Quality
    runs-on: ubuntu-latest
    outputs:
      quality_passed: ${{ steps.quality_check.outputs.passed }}

    steps:
      - uses: actions/checkout@v4

      - name: Install pmat
        run: |
          cargo install pmat --locked

      - name: Generate comprehensive report
        id: generate_report
        run: |
          pmat report --json > release-quality.json

          # Extract key metrics
          TOTAL_DEFECTS=$(jq '.summary.total_defects' release-quality.json)
          CRITICAL=$(jq '.summary.by_severity.critical // 0' release-quality.json)
          HIGH=$(jq '.summary.by_severity.high // 0' release-quality.json)

          echo "total_defects=$TOTAL_DEFECTS" >> $GITHUB_OUTPUT
          echo "critical_defects=$CRITICAL" >> $GITHUB_OUTPUT
          echo "high_defects=$HIGH" >> $GITHUB_OUTPUT

      - name: Quality gate check
        id: quality_check
        run: |
          TOTAL=${{ steps.generate_report.outputs.total_defects }}
          CRITICAL=${{ steps.generate_report.outputs.critical_defects }}
          HIGH=${{ steps.generate_report.outputs.high_defects }}

          if [ "$CRITICAL" -gt 0 ]; then
            echo "âŒ Release blocked: $CRITICAL critical defects found"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          elif [ "$HIGH" -gt 5 ]; then
            echo "âŒ Release blocked: $HIGH high severity defects exceed limit (5)"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          elif [ "$TOTAL" -gt 50 ]; then
            echo "âš ï¸  Warning: $TOTAL total defects found (limit: 50)"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "âœ… Quality gates passed: $TOTAL defects within limits"
            echo "passed=true" >> $GITHUB_OUTPUT
          fi

      - name: Create release report
        if: always()
        uses: softprops/action-gh-release@v1
        with:
          files: |
            release-quality.json
          body: |
            ## Release Quality Report

            - Total Defects: ${{ steps.generate_report.outputs.total_defects }}
            - Critical: ${{ steps.generate_report.outputs.critical_defects }}
            - High: ${{ steps.generate_report.outputs.high_defects }}

            Quality Gate: ${{ steps.quality_check.outputs.passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}
```

#### D. Matrix Testing with Platform-Specific Analysis

**`.github/workflows/platform-matrix.yml`**

```yaml
name: Platform Matrix Testing
on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  platform-analysis:
    name: Analysis on ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        rust: [stable, beta]
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
          - os: windows-latest
            target: x86_64-pc-windows-msvc
          - os: macos-latest
            target: x86_64-apple-darwin

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ matrix.rust }}
          target: ${{ matrix.target }}

      - name: Platform-specific setup
        shell: bash
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            echo "PMAT_TEMP=${{ runner.temp }}" >> $GITHUB_ENV
          elif [[ "${{ matrix.os }}" == "macos-latest" ]]; then
            echo "PMAT_CACHE_DIR=$HOME/Library/Caches/pmat" >> $GITHUB_ENV
          else
            echo "PMAT_CACHE_DIR=$HOME/.cache/pmat" >> $GITHUB_ENV
          fi

      - name: Build and test
        run: |
          cargo build --release --target ${{ matrix.target }}
          cargo test --release --target ${{ matrix.target }}

      - name: Run platform-specific analysis
        shell: bash
        run: |
          # Use built binary
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            PMAT_BIN="target/${{ matrix.target }}/release/pmat.exe"
          else
            PMAT_BIN="target/${{ matrix.target }}/release/pmat"
          fi

          # Basic functionality test
          $PMAT_BIN --version

          # Run analysis
          $PMAT_BIN analyze complexity --top-files 5 --format json > complexity-${{ matrix.os }}.json
          $PMAT_BIN report --json > report-${{ matrix.os }}.json

      - name: Upload platform results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.os }}-${{ matrix.rust }}
          path: |
            complexity-${{ matrix.os }}.json
            report-${{ matrix.os }}.json
```

#### E. Performance Regression Detection

**`.github/workflows/performance-regression.yml`**

```yaml
name: Performance Regression Detection
on:
  pull_request:
    paths:
      - 'server/src/**'
      - 'Cargo.toml'

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest-16-cores

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout base branch
        run: |
          git fetch origin ${{ github.base_ref }}
          git checkout origin/${{ github.base_ref }}
          cp -r . ../base-branch
          git checkout -

      - name: Setup benchmark environment
        run: |
          # Disable CPU frequency scaling for consistent benchmarks
          echo "performance" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

          # Disable turbo boost
          echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo

      - name: Run baseline benchmarks
        working-directory: ../base-branch
        run: |
          cargo build --release

          # Warm up
          for i in {1..3}; do
            timeout 60s target/release/pmat analyze complexity . --top-files 100 > /dev/null
          done

          # Actual benchmark
          hyperfine \
            --warmup 3 \
            --runs 10 \
            --export-json ../baseline.json \
            --export-markdown ../baseline.md \
            'target/release/pmat analyze complexity . --top-files 100' \
            'target/release/pmat analyze comprehensive --format json' \
            'target/release/pmat report --json'

      - name: Run PR benchmarks
        run: |
          cargo build --release

          # Warm up
          for i in {1..3}; do
            timeout 60s target/release/pmat analyze complexity . --top-files 100 > /dev/null
          done

          # Actual benchmark
          hyperfine \
            --warmup 3 \
            --runs 10 \
            --export-json pr.json \
            --export-markdown pr.md \
            'target/release/pmat analyze complexity . --top-files 100' \
            'target/release/pmat analyze comprehensive --format json' \
            'target/release/pmat report --json'

      - name: Compare benchmarks
        run: |
          # Install comparison tool
          cargo install hyperfine-compare

          # Generate comparison
          hyperfine-compare ../baseline.json pr.json > comparison.md

          # Check for regressions (>10% slower)
          python3 -c "
          import json
          import sys

          with open('../baseline.json') as f:
              baseline = json.load(f)
          with open('pr.json') as f:
              pr = json.load(f)

          regressions = []
          for b, p in zip(baseline['results'], pr['results']):
              baseline_mean = b['mean']
              pr_mean = p['mean']

              if pr_mean > baseline_mean * 1.1:  # 10% regression threshold
                  percent = ((pr_mean - baseline_mean) / baseline_mean) * 100
                  regressions.append(f\"{b['command']}: {percent:.1f}% slower\")

          if regressions:
              print('âŒ Performance regressions detected:')
              for r in regressions:
                  print(f'  - {r}')
              sys.exit(1)
          else:
              print('âœ… No performance regressions detected')
          "

      - name: Comment benchmark results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comparison = fs.readFileSync('comparison.md', 'utf8');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `## âš¡ Performance Benchmark Results\n\n${comparison}`
            });
```

#### F. Incremental Analysis for Large Repositories

**`.github/workflows/incremental-analysis.yml`**

```yaml
name: Incremental Analysis
on:
  pull_request:
    types: [opened, synchronize]

jobs:
  changed-files-analysis:
    name: Analyze Changed Files
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v41
        with:
          files: |
            **/*.rs
            **/*.ts
            **/*.py
          separator: '\n'

      - name: Install pmat
        if: steps.changed-files.outputs.any_changed == 'true'
        run: cargo install pmat --locked

      - name: Analyze changed files
        if: steps.changed-files.outputs.any_changed == 'true'
        run: |
          # Create file list
          echo "${{ steps.changed-files.outputs.all_changed_files }}" > changed_files.txt

          # Run targeted analysis
          while IFS= read -r file; do
            echo "Analyzing $file..."

            # Complexity analysis for single file
            pmat analyze complexity "$file" --format json >> complexity_results.jsonl

            # SATD detection
            pmat analyze satd "$file" --format json >> satd_results.jsonl

            # Big-O analysis if applicable
            if [[ "$file" == *.rs ]] || [[ "$file" == *.ts ]]; then
              pmat analyze big-o "$file" --format json >> bigo_results.jsonl || true
            fi
          done < changed_files.txt

          # Aggregate results
          jq -s '.' complexity_results.jsonl > incremental_complexity.json
          jq -s '.' satd_results.jsonl > incremental_satd.json
          jq -s '.' bigo_results.jsonl > incremental_bigo.json 2>/dev/null || echo '[]' > incremental_bigo.json

      - name: Generate incremental report
        if: steps.changed-files.outputs.any_changed == 'true'
        run: |
          # Custom Python script to merge incremental results
          python3 << 'EOF'
          import json
          import sys
          from datetime import datetime

          # Load incremental results
          with open('incremental_complexity.json') as f:
              complexity = json.load(f)
          with open('incremental_satd.json') as f:
              satd = json.load(f)
          with open('incremental_bigo.json') as f:
              bigo = json.load(f)

          # Generate summary
          total_issues = len(complexity) + len(satd) + len(bigo)

          report = {
              "metadata": {
                  "type": "incremental",
                  "generated_at": datetime.utcnow().isoformat(),
                  "files_analyzed": len(set(
                      [item.get('file_path', '') for item in complexity] +
                      [item.get('file_path', '') for item in satd] +
                      [item.get('file_path', '') for item in bigo]
                  ))
              },
              "summary": {
                  "total_issues": total_issues,
                  "complexity_issues": len(complexity),
                  "satd_issues": len(satd),
                  "bigo_issues": len(bigo)
              },
              "details": {
                  "complexity": complexity,
                  "satd": satd,
                  "big_o": bigo
              }
          }

          with open('incremental_report.json', 'w') as f:
              json.dump(report, f, indent=2)

          # Generate markdown summary
          md = f"""## ðŸ“Š Incremental Analysis Report

          Analyzed **{report['metadata']['files_analyzed']}** changed files.

          ### Summary
          - **Total Issues**: {total_issues}
          - **Complexity Issues**: {len(complexity)}
          - **Technical Debt**: {len(satd)}
          - **Performance Concerns**: {len(bigo)}

          ### Details
          """

          if complexity:
              md += "\n#### Complexity Issues\n"
              for item in complexity[:5]:  # Top 5
                  md += f"- `{item.get('file_path', 'unknown')}`: {item.get('message', 'N/A')}\n"

          if satd:
              md += "\n#### Technical Debt\n"
              for item in satd[:5]:
                  md += f"- `{item.get('file_path', 'unknown')}:{item.get('line_start', 0)}`: {item.get('message', 'N/A')}\n"

          if bigo:
              md += "\n#### Performance Analysis\n"
              for item in bigo[:5]:
                  md += f"- `{item.get('function_name', 'unknown')}`: {item.get('time_complexity', 'N/A')}\n"

          with open('incremental_report.md', 'w') as f:
              f.write(md)

          # Exit with error if critical issues
          critical = sum(1 for item in complexity if item.get('severity') == 'critical')
          if critical > 0:
              print(f"âŒ {critical} critical issues found")
              sys.exit(1)
          EOF

      - name: Comment PR with incremental results
        if: steps.changed-files.outputs.any_changed == 'true' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('incremental_report.md', 'utf8');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: report
            });
```

### Key GitHub Actions Optimizations

1. **Intelligent Caching**: Leverages `actions/cache@v4` and `Swatinem/rust-cache@v2` for 5-10x faster builds
2. **Matrix Parallelization**: Distributes analysis across multiple jobs for O(1/n) time complexity
3. **High-Performance Runners**: Uses `ubuntu-latest-16-cores` for CPU-intensive analysis
4. **Incremental Analysis**: Only analyzes changed files in PRs, reducing runtime from O(n) to O(k)
5. **Artifact Management**: Stores results with retention policies to balance storage costs
6. **SARIF Integration**: Native GitHub Security tab integration for defect tracking
7. **Hyperfine Benchmarking**: Statistically rigorous performance regression detection with 10 runs
8. **Platform-Specific Optimization**: Tailored configurations for Linux/Windows/macOS environments

These workflows demonstrate production-grade CI/CD patterns with measured performance improvements:
- PR validation: 2-3 minutes (was 15+ minutes)
- Nightly analysis: 15-20 minutes for 100K+ LOC
- Incremental analysis: <1 minute for typical PRs

*This specification establishes the `report` command as the primary defect aggregation interface while ensuring all analysis commands provide complete file context.*
